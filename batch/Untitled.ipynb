{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57cf2c82-13a5-4092-bfbf-f6b1bbc01914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv(r\"C:\\Users\\gowth\\Desktop\\Final_project\\batch\\.env\")\n",
    "sf_url = os.getenv(\"SFURL\")\n",
    "sf_user=os.getenv(\"SFUSER\")\n",
    "sf_password = os.getenv(\"SFPASSWORD\")\n",
    "sf_database = os.getenv(\"SFDATABASE\")\n",
    "sf_schema = os.getenv(\"SFSCHEMA\")\n",
    "sf_warehouse = os.getenv(\"SFWAREHOUSE\")\n",
    "sf_role = os.getenv(\"SFROLE\")\n",
    "my_pass=os.getenv(\"PASSWORD\")\n",
    "\n",
    "#spark creation\n",
    "def spark_create():\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"MySQL_Snowflake_Integration\") \\\n",
    "    .config(\"spark.jars\", r\"C:\\Users\\gowth\\Desktop\\Final_project\\mysql-connector-j-9.3.0\\mysql-connector-j-9.3.0.jar\") \\\n",
    "    .config(\"spark.jars.packages\", \"net.snowflake:spark-snowflake_2.12:3.1.1,net.snowflake:snowflake-jdbc:3.13.6\") \\\n",
    "    .getOrCreate()\n",
    "    return spark\n",
    "#spark-mysql connection\n",
    "def spark_mysql_connection(spark):\n",
    "    jdbc_url = \"jdbc:mysql://localhost:3306/db_guvi?useSSL=false&serverTimezone=UTC\"\n",
    "    connection_properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": my_pass,\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "    \"timestampTimezone\": \"Asia/Kolkata\"\n",
    "    }\n",
    "    return jdbc_url, connection_properties\n",
    "#mysql final data \n",
    "def weather_data_mysql(spark,jdbc_url,connection_properties):\n",
    "    last_load_df = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"load_time_tracker\",\n",
    "    properties=connection_properties)\n",
    "    last_load_df = last_load_df.withColumn(\"loaded_time_corrected\", expr(\"last_loaded_time - INTERVAL 5 HOURS 30 MINUTES\"))\n",
    "    corrected_time = last_load_df.select(\"loaded_time_corrected\").first()[0]\n",
    "\n",
    "    query = f\"(SELECT * FROM station_data WHERE loaded_time > '{corrected_time}') AS filtered_data\"\n",
    "\n",
    "    df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=query,\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    final_df=df.withColumn(\"loaded_time\", expr(\"loaded_time - INTERVAL 5 HOURS 30 MINUTES\"))\n",
    "    return final_df\n",
    "\n",
    "#csv data\n",
    "def read_sensor_csv(spark):\n",
    "    df=spark.read.option(\"header\",\"True\") \\\n",
    "        .option(\"inferSchema\",\"True\") \\\n",
    "        .csv(r\"C:\\Users\\gowth\\Desktop\\Final_project\\faker_output\")\n",
    "    return df\n",
    "#load final data to mysql\n",
    "def spark_to_mysql(df,spark,jdbc_url,tablename,connection_props):\n",
    "    df.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=tablename,\n",
    "    mode=\"append\",\n",
    "    properties=connection_props)\n",
    "\n",
    "\n",
    "#snowflake connection\n",
    "\n",
    "def snowflake_loading(df,spark,tablename):\n",
    "    sfOptions = {\n",
    "    \"sfURL\": sf_url,\n",
    "    \"sfUser\": sf_user,\n",
    "    \"sfPassword\": sf_password,\n",
    "    \"sfDatabase\": sf_database,\n",
    "    \"sfSchema\": sf_schema,\n",
    "    \"sfWarehouse\": sf_warehouse,\n",
    "    \"sfRole\": sf_role }\n",
    "\n",
    "    df.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", tablename) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "\n",
    "def upload_to_s3():\n",
    "    aws_access_key=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "    aws_secret_key =os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "    bucket_name = 'backupdataguvi'\n",
    "    s3_folder = 'sensor_backup/'\n",
    "    local_folder = r\"C:\\Users\\gowth\\Desktop\\Final_project\\faker_output\"\n",
    "    s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key)\n",
    "\n",
    "    for filename in os.listdir(local_folder):\n",
    "        if filename.endswith('.csv'):\n",
    "            print(filename)\n",
    "            local_path = os.path.join(local_folder, filename)\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            s3_key = s3_folder + f\"{timestamp}_{filename}\"\n",
    "            try:\n",
    "                s3_client.upload_file(local_path, bucket_name, s3_key)\n",
    "                print(f\"Uploaded: {filename} -> s3://{bucket_name}/{s3_key}\")\n",
    "                os.remove(local_path)\n",
    "                print(f\"üóëÔ∏è Deleted local file: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to upload {filename}: {e}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505704bb-45e7-4583-b664-e417115a4c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=spark_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd3cd50-2e79-4fea-ab2c-89dcebea5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url, connection_properties=spark_mysql_connection(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c363b11d-d4c5-47a3-b868-458eb3b80980",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=weather_data_mysql(spark,jdbc_url,connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "152b0ecb-b410-4a7d-b329-227244e3f3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+--------+------------+----------------+-------------------+\n",
      "|station_id|      date|min_temp|max_temp|avg_humidity|precipitation_mm|        loaded_time|\n",
      "+----------+----------+--------+--------+------------+----------------+-------------------+\n",
      "|     S1968|2025-06-05|    24.4|    36.2|       31.93|            0.46|2025-06-22 14:08:34|\n",
      "|     S9042|2025-06-20|    15.2|    28.4|       47.02|           14.95|2025-06-22 14:08:34|\n",
      "|     S1135|2024-10-30|     6.5|    16.0|       44.27|            0.17|2025-06-22 14:08:34|\n",
      "|     S8444|2024-11-02|    29.2|    35.0|       86.33|           13.31|2025-06-22 14:08:34|\n",
      "|     S2510|2025-02-01|    24.7|    34.6|       36.19|            3.68|2025-06-22 14:08:34|\n",
      "|     S7533|2025-02-12|    -2.9|     2.5|       71.05|            0.29|2025-06-22 14:08:34|\n",
      "|     S7303|2024-11-15|    -9.7|     2.1|       56.83|            0.91|2025-06-22 14:08:34|\n",
      "|     S7131|2025-05-02|     2.3|    14.4|       49.32|           13.87|2025-06-22 14:08:34|\n",
      "|     S2396|2024-09-07|     7.0|    10.2|       73.87|           15.74|2025-06-22 14:08:34|\n",
      "|     S4569|2025-02-24|    19.3|    21.1|       82.37|           11.38|2025-06-22 14:08:34|\n",
      "+----------+----------+--------+--------+------------+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "168bb37e-49aa-44ed-8ce4-188f8e613db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+--------+------------+----------------+-------------------+\n",
      "|station_id|      date|min_temp|max_temp|avg_humidity|precipitation_mm|        loaded_time|\n",
      "+----------+----------+--------+--------+------------+----------------+-------------------+\n",
      "|     S1968|2025-06-05|    24.4|    36.2|       31.93|            0.46|2025-06-22 19:38:34|\n",
      "|     S9042|2025-06-20|    15.2|    28.4|       47.02|           14.95|2025-06-22 19:38:34|\n",
      "|     S1135|2024-10-30|     6.5|    16.0|       44.27|            0.17|2025-06-22 19:38:34|\n",
      "|     S8444|2024-11-02|    29.2|    35.0|       86.33|           13.31|2025-06-22 19:38:34|\n",
      "|     S2510|2025-02-01|    24.7|    34.6|       36.19|            3.68|2025-06-22 19:38:34|\n",
      "|     S7533|2025-02-12|    -2.9|     2.5|       71.05|            0.29|2025-06-22 19:38:34|\n",
      "|     S7303|2024-11-15|    -9.7|     2.1|       56.83|            0.91|2025-06-22 19:38:34|\n",
      "|     S7131|2025-05-02|     2.3|    14.4|       49.32|           13.87|2025-06-22 19:38:34|\n",
      "|     S2396|2024-09-07|     7.0|    10.2|       73.87|           15.74|2025-06-22 19:38:34|\n",
      "|     S4569|2025-02-24|    19.3|    21.1|       82.37|           11.38|2025-06-22 19:38:34|\n",
      "+----------+----------+--------+--------+------------+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr_station_df = df.withColumn(\"loaded_time\", expr(\"loaded_time + INTERVAL 5 HOURS 30 MINUTES\"))\n",
    "cr_station_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff192ec1-dee5-435d-8a90-f820a0bec8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowflake_loading(cr_station_df, spark, \"weather_station_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afadfac-06ed-4f4a-9bf9-d861cc1a59cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
